---
categories:
  - Java
  - 知识库
  - Java
---
- 多线程操作共享变量，变量值不一致。

- 使用volatile关键字修饰共享变量，保证共享变量在线程之间的可见性。

  - synchronized 关键字也能保证共享变量在多个线程之间的可见性
  - CAS 机制也能保证共享变量在多个线程之间的可见性

- CPU读写模型是引发变量可见性的原因

  -  主存RAM存取速度比 CPU操作慢得多，为避免CPU高速处理能力不能充分发挥作用，所以CPU处理器引入了高速缓冲存储器。（造价高，容量较主存小）
  - 程序局部性原理（？？？）：CPU试图访问主存的某一单元时，会认为临近单元后续被利用的可能性很大，所以CPU在存取主存单元时，计算机硬件会自动把包括该单元在内的那一组单元调入高速缓存，之后CPU就可以直接对高速缓存数据进行存取。（一组单元称为内存块`block`，通常是连续的64个字节）

- CPU Cache相关术语

  - Cache Line & Slot & Hot Data & Cache Hit
    - Cache Line = Slot 
    - 高速缓存会将其存储单元均分成若干等份，每一等份就是一个缓存行，如今主流CPU的缓存行一般都是64个字节
    - 被缓存行缓存的数据称之为热点数据（hot data）
  - Cache Hit & Cache Miss & Hit Latency
    - Cache Hit：当CPU通过`寄存器`中存储的数据地址请求访问数据时（包括读操作和写操作），首先会在Cache中查找，如果找到了则直接返回Cache中存储的数据，这称为**缓存命中**（cache hit），根据操作类型又可分为读缓存命中和写缓存命中。
    - Hit Latency：命中延迟是指判断Cache中是否缓存了目标数据所花的时间。
  - Cache分级：速度— L1 > L2 > L3（L3在CPU的多个核心之间共享）
  - Cache替换算法 & Cache Line Conflict
    - 常用的算法：“最近最少使用算法”（LRU算法）least recently used；原理：为每行设置一个计数器，把名中行的计数器清零，其他各行计数器加1，当需要替换时淘汰行计数器计数值最大的数据行出局
  - 时钟周期：完成一次存取操作所需的时间—主频倒数（寄存器、高速缓存、主存都有主频）

- Cache结构设计和缓存关联性（`关联主内存`）

  - **CPU高速缓存组成**：整个Cache被分为S个组，每个组又有E行个最小的存储单元——Cache Line；而一个Cache Line中有B（B=64）个字节用来存储数据，即每个Cache Line能存储64个字节的数据
  - **Cache Line组成**：包含1个有效位（valid bit）、t个标记位（tag bit）、数据
    - — valid bit用来表示该缓存行是否有效；tag bit用来协助寻址，唯一标识存储在Cache Line中的块；而Cache Line里的64个字节其实是对应内存地址中的数据拷贝。
  - 将高速缓存内的数据映射到主内存的数据（**缓存关联性**）
    - 完全关联(Fully associative cache) 
      - 以32位操作系统（意味着到内存寻址时是通过32位地址）为例，，比如有一个0101...10 000000 - 0101...10 111111（为了节省版面省略了高26位中的部分bit位，这个区间代表高26位相同但低6位不同的64个地址，即64字节的内存块（主内存的存储单元：连续的64个字节））内存块需要缓存，那么它将会被随机存放到一个可用的Slot中，并将高26位作为该Slot的tag bit（前文说到每行除了存储内存块的64字节Cache Line，还额外有1个bit标识该行是否有效和t个bit作为该行的唯一ID，本例中t就是26）。这样当内存需要存取这个地址范围内的数据地址时，首先会去Cache中找是否缓存了高26位（tag bit）为0101...10的Slot，如果找到了再根据数据地址的低6位定位到Cache Line的某个存储单元上，这个低6位称为字节偏移（word offset）。
        - 6位字节偏移(2^6=64个字节)——**主内存的存储单元block的大小固定位64个字节**
        - 26位 tag bit 作为数据块唯一标识，用于寻址时轮询所有缓存行(slot)并匹配数据块
    - 直接映射(Direct mapped cache) 
      - 包含tag bit 位数据地址 + slot offset(定位slot) + word offset 字节偏移（缓存行）
      - 对于给定的32位数据地址，首先不管低6位，取出中间的`slot offset`个bit位，定位出是哪一个Slot，然后比较该Slot的`tag bit`是否和数据地址的剩余高位匹配，如果匹配那么表示Cache Hit，最后在根据低6位从该Slot的Cache Line中找到具体的存储单元进行存取数据。
      - 缺点：低位相同但高位不同的内存块会被映射到同一个Slot上（因为对SlotCount取模之后结果相同），如果碰巧CPU请求存取这些内存块，那么将只有一个内存块能够被缓存到Cache中对应的Slot上，也就是说容易发生Cache Line Conflict。
    - N路组关联(N-way set associative cache) 
      - 对Direct Mapped Cache和Full Associative Cache的一个结合，思路是不要对于给定的数据地址就定死了放在哪个Slot上。
      - 先将Cache均分成S个组，每个组都有E个Slot。假设将我的L1 Cache 128KB按16个Slot划分为一个组，那么组数为：`128 * 1024 / 64`（Slot数）/ 16 = 128 个组（我们将每个组称为一个Set，表示一组Slot的集合）；与Direct Mapped Cache不同的地方就是将原本表示映射到哪个Slot的11个中间bit位改成了用7个bit位表示映射到哪个Set上，在确定Set之后，内存块将被放入该Set的哪个Slot是随机的（可能当时哪个可以用就放到哪个了），然后以剩余的高位19个bit位作为最终存放该内存块的`tag bit`。
      - 对于一个给定的数据地址只会将其映射到特定的Set上，这样就大大减小了Cache Line Conflict的几率，并且CPU在查找Slot时只需在具体的某个Set中线性查找，而Set中的Slot个数较少（分组分得越多，每个组的Slot就越少），这样线性查找的时间复杂度也近似O(1)了。

- 如何编写对Cache Hit友好的程序？为提高Cache命中率，要充分发挥局部性原理。

  - **时间局部性**：对于同一数据可能被多次使用，自第一次加载到Cache Line后，后面的访问就可以多次从Cache Line中命中，从而提高读取速度（而不是从下层缓存读取）。

  - **空间局部性**：一个Cache Line有64字节块，我们可以充分利用一次加载64字节的空间，把程序后续会访问的数据，一次性全部加载进来，从而提高Cache Line命中率（而不是重新去寻址读取）。

  - 实际操作：读取时尽量读取相邻的数据地址

    - 以上操作带来的负面影响：造成缓存一致性问题。

    - 解决方案：（缓存一致性协议）

      - ```
        这是一个跟踪每个缓存行的状态的缓存子系统。该系统使用一个称为 “总线动态监视” 或者称为*“总线嗅探”* 的技术来监视在系统总线上发生的所有事务，以检测缓存中的某个地址上何时发生了读取或写入操作。
        当这个缓存子系统在系统总线上检测到对缓存中加载的内存区域进行的读取操作时，它会将该缓存行的状态更改为 “shared”。如果它检测到对该地址的写入操作时，会将缓存行的状态更改为 “invalid”。
        该缓存子系统想知道，当该系统在监视系统总线时，系统是否在其缓存中包含数据的惟一副本。如果数据由它自己的 CPU 进行了更新，那么这个缓存子系统会将缓存行的状态从 “exclusive” 更改为 “modified”。如果该缓存子系统检测到另一个处理器对该地址的读取，它会阻止访问，更新系统内存中的数据，然后允许该处理的访问继续进行。它还允许将该缓存行的状态标记为 shared。
        ```

      - 各CPU都会通过总线嗅探来监视其他CPU，一旦某个CPU对自己Cache中缓存的共享变量做了修改（能做修改的前提是共享变量所在的缓存行的状态不是无效的），那么就会导致其他缓存了该共享变量的CPU将该变量所在的Cache Line置为无效状态，在下次CPU访问无效状态的缓存行时会首先要求对共享变量做了修改的CPU将修改从Cache写回主存，然后自己再从主存中将最新的共享变量读到自己的缓存行中。（这里的CPU指的是CPU核心）

      - 并且，缓存一致性协议通过缓存锁定来保证CPU修改缓存行中的共享变量并通知其他CPU将对应缓存行置为无效这一操作的原子性，即当某个CPU修改位于自己缓存中的共享变量时会禁止其他也缓存了该共享变量的CPU访问自己缓存中的对应缓存行，并在缓存锁定结束前通知这些CPU将对应缓存行置为无效状态。

      - 在缓存锁定出现之前，是通过总线锁定来实现CPU之间的同步的，即CPU在回写主存时会锁定总线不让其他CPU访问主存，但是这种机制开销较大，一个CPU对共享变量的操作会导致其他CPU对其他共享变量的访问。(锁粒度由总线细化到CPU核心)

    - 缓存一致性协议带来的问题：**伪共享**——不同CPU核心对同一缓存行的不同字节数据的操作可能会导致缓存锁定（变成串行程序，降低了并发性（乒乓效应））

    - 伪共享问题解决方案：

      - 缓存行填充（Cache Line Padding），通过增加两个变量的地址距离使之位于两个不同的缓存行上，如此对共享变量X和Y的操作不会相互影响。
      - 线程不直接操作全局共享变量，而是将全局共享变量读取一份副本到自己的局部变量，局部变量在线程之间是不可见的因此随你线程怎么玩，最后线程再将玩出来的结果写回全局变量。（再进行锁定）

- 并发编程的三要素是：原子性、可见性、有序性。

- **锁释放有着volatile域写语义**？？ & 原子类CAS更新有着volatile域写语义

- Unsafe类中的CAS自旋

  - CAS操作在x86上是由cmpxchg（Compare Exchange）实现的（不同指令集有所不同）
  - JDK提供的AtomicXxx`系列原子操作类已能满足大多数需求
  - **原子类封装了一个volatile域**：将改写立即刷新到主存；将改写立即刷新到主存

- volatile禁止重排序

  - volatile产生的汇编指令lock具有个指令屏障使得该屏障之前的指令不能重排序到屏障之后。
  - volatile的作用案例：使用单例模式的并发优化案例
    - 懒加载模式
      - 利用类加载过程的初始化阶段会执行**类构造器**<clinit>，按照显式声明为**静态变量**初始化的特点。
      - **当类被主动引用时应当立即对其初始化**！！！
      - 对类的主动引用：
        - new、getStatic、putStatic、invokeStatic
        - 通过java.lang.reflect包的方法对该类进行反射调用时
        - 当初始化一个类时，如果他的父类没被初始化，那么先初始化其父类
        - 当JVM启动时，首先会初始化main函数所在的类
      - 对类的被动引用：
        - 通过子类访问父类静态变量，子类不会被立即初始化
        - 通过数组定义引用的类不会被立即初始化
        - 访问某个类的常量，该类不会被立即初始化（因为经过编译阶段的常量传播优化，该常量已被复制一份到当前类的常量池中了）
    - 饿汉模式1
    - 饿汉模式2
    - DoubleCheckedLocking
    - DCL2
    - InstanceHolder
    - 枚举实例的构造器只会被调用一次



